{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6c2a0c-425a-40ec-a9a2-7d88ed7d1403",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import textwrap\n",
    "import sys\n",
    "import langchain\n",
    "import torch\n",
    "import faiss\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import TextLoader, UnstructuredPDFLoader, PyPDFLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.chat_models.huggingface import ChatHuggingFace\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import pipeline\n",
    "from huggingface_hub import notebook_login\n",
    "from langchain import HuggingFacePipeline, PromptTemplate\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_community.document_loaders import UnstructuredPDFLoader, DirectoryLoader\n",
    "from pdf2image import convert_from_path\n",
    "import os\n",
    "import textwrap\n",
    "import torch\n",
    "import langchain\n",
    "import langchain_core\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.chat_models.huggingface import ChatHuggingFace\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pdf2image import convert_from_path\n",
    "from qdrant_client import qdrant_client\n",
    "from qdrant_client import QdrantClient\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "import torch\n",
    "import langchain\n",
    "import langchain_core\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a89857a-aa8a-4205-b824-d2cb18f84555",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"QDRANT_HOST\"] = \"https://ef3d190d-4471-4044-b8a5-6aba8c656aa8.us-east4-0.gcp.cloud.qdrant.io:6333\"\n",
    "os.environ[\"QDRANT_API_KEY\"] = \"EPOl1GIG9WTr2joExRzwjRkk5LuLWNvImfgU4y2GPaN_Nu5kWcs51w\"\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301ee911-d450-457e-831e-ae90677347fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = qdrant_client.QdrantClient(\n",
    "    url=os.getenv(\"QDRANT_HOST\"),\n",
    "    api_key = os.getenv(\"QDRANT_API_KEY\")\n",
    ")\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name = 'keepitreal/vietnamese-sbert')\n",
    "doc_store = Qdrant(\n",
    "    client=client, collection_name=\"Pengi-Doc\", \n",
    "    embeddings=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3834d4c4-1a5c-4b3c-bb4f-81866bb7c7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"vilm/vinallama-7b-chat\", token = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\"))\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", token=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"vilm/vinallama-7b-chat\",\n",
    "                                             device_map='cuda:0',\n",
    "                                             torch_dtype=torch.float16,\n",
    "                                            #  load_in_4bit=True\n",
    "                                             load_in_8bit=True,\n",
    "                                             use_auth_token=os.getenv(\"HUGGINGFACEHUB_API_TOKEN\"))\n",
    "\n",
    "\n",
    "pipe = pipeline(\"text-generation\",\n",
    "                model= model,\n",
    "                tokenizer= tokenizer,\n",
    "                torch_dtype= torch.bfloat16,\n",
    "                device_map= \"auto\",\n",
    "                max_new_tokens = 512,\n",
    "                do_sample = True,\n",
    "                top_k = 15,\n",
    "                num_return_sequences=1,\n",
    "                eos_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe, model_kwargs={\"temperature\": 0.05})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9ffe96-48be-43e6-bf4c-fb85575f25fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"<|im_start|>system\\nSử dụng thông tin sau đây để trả lời câu hỏi. Nếu bạn không biết câu trả lời, hãy nói không biết, đừng cố tạo ra câu trả lời\\n\n",
    "{context}<|im_end|>\\n<|im_start|>user\\n{question}<|im_end|>\\n<|im_start|>assitant\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type='stuff',  # Change 'stuff' to a valid chain type\n",
    "    retriever=doc_store.as_retriever(search_kwargs={\"k\": 3}),\n",
    "    verbose=True,\n",
    "    chain_type_kwargs={\n",
    "        \"verbose\": True,\n",
    "        \"prompt\": QA_CHAIN_PROMPT\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6d0b02-ec8b-4ff3-8bf8-543d435bca30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(device = torch.device('cuda' if torch.cuda.is_available() else 'cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b243fb30-37a4-420d-a3e5-96165c513f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# start_time = time.time()\n",
    "# print(qa_chain.invoke(\"Chương trình đào tạo của chuyên ngành trí tuệ nhân tạo?\"))\n",
    "# print(device = torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "# print(\"--- Inference time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4898c4c8-f9ae-4852-a891-4e8aef17bac5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu",
   "language": "python",
   "name": "pytorch_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
