{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6c2a0c-425a-40ec-a9a2-7d88ed7d1403",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import textwrap\n",
    "import sys\n",
    "import langchain\n",
    "import torch\n",
    "import faiss\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import TextLoader, UnstructuredPDFLoader, PyPDFLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.chat_models.huggingface import ChatHuggingFace\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import pipeline\n",
    "from huggingface_hub import notebook_login\n",
    "from langchain import HuggingFacePipeline, PromptTemplate\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_community.document_loaders import UnstructuredPDFLoader, DirectoryLoader\n",
    "from pdf2image import convert_from_path\n",
    "import os\n",
    "import textwrap\n",
    "import torch\n",
    "import langchain\n",
    "import langchain_core\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.chat_models.huggingface import ChatHuggingFace\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pdf2image import convert_from_path\n",
    "from qdrant_client import qdrant_client\n",
    "from qdrant_client import QdrantClient\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "import torch\n",
    "import langchain\n",
    "import langchain_core\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a89857a-aa8a-4205-b824-d2cb18f84555",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"QDRANT_HOST\"] = \"https://ef3d190d-4471-4044-b8a5-6aba8c656aa8.us-east4-0.gcp.cloud.qdrant.io:6333\"\n",
    "os.environ[\"QDRANT_API_KEY\"] = \"EPOl1GIG9WTr2joExRzwjRkk5LuLWNvImfgU4y2GPaN_Nu5kWcs51w\"\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301ee911-d450-457e-831e-ae90677347fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = qdrant_client.QdrantClient(\n",
    "    url=os.getenv(\"QDRANT_HOST\"),\n",
    "    api_key = os.getenv(\"QDRANT_API_KEY\")\n",
    ")\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name = 'keepitreal/vietnamese-sbert')\n",
    "doc_store = Qdrant(\n",
    "    client=client, collection_name=\"Pengi-Doc\", \n",
    "    embeddings=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3834d4c4-1a5c-4b3c-bb4f-81866bb7c7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"vilm/vinallama-7b-chat\", token = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\"))\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", token=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"vilm/vinallama-7b-chat\",\n",
    "                                             device_map='cuda:0',\n",
    "                                             torch_dtype=torch.float16,\n",
    "                                            #  load_in_4bit=True\n",
    "                                             load_in_8bit=True,\n",
    "                                             use_auth_token=os.getenv(\"HUGGINGFACEHUB_API_TOKEN\"))\n",
    "\n",
    "\n",
    "pipe = pipeline(\"text-generation\",\n",
    "                model= model,\n",
    "                tokenizer= tokenizer,\n",
    "                torch_dtype= torch.bfloat16,\n",
    "                device_map= \"auto\",\n",
    "                max_new_tokens = 512,\n",
    "                do_sample = True,\n",
    "                top_k = 15,\n",
    "                num_return_sequences=1,\n",
    "                eos_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe, model_kwargs={\"temperature\": 0.05})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9ffe96-48be-43e6-bf4c-fb85575f25fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"<|im_start|>system\\nSử dụng thông tin sau đây để trả lời câu hỏi. Nếu bạn không biết câu trả lời, hãy nói không biết, đừng cố tạo ra câu trả lời\\n\n",
    "{context}<|im_end|>\\n<|im_start|>user\\n{question}<|im_end|>\\n<|im_start|>assitant\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type='stuff',  # Change 'stuff' to a valid chain type\n",
    "    retriever=doc_store.as_retriever(search_kwargs={\"k\": 3}),\n",
    "    verbose=True,\n",
    "    chain_type_kwargs={\n",
    "        \"verbose\": True,\n",
    "        \"prompt\": QA_CHAIN_PROMPT\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6d0b02-ec8b-4ff3-8bf8-543d435bca30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(device = torch.device('cuda' if torch.cuda.is_available() else 'cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b243fb30-37a4-420d-a3e5-96165c513f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# start_time = time.time()\n",
    "# print(qa_chain.invoke(\"Chương trình đào tạo của chuyên ngành trí tuệ nhân tạo?\"))\n",
    "# print(device = torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "# print(\"--- Inference time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4898c4c8-f9ae-4852-a891-4e8aef17bac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "templates=[\"\"\"<|im_start|>system\\nBạn là một trợ lý AI hữu ích. Bạn chỉ sử dụng những thông tin mà bạn được cung cấp để trả lời các câu hỏi,\n",
    "hãy trả lời câu hỏi một cách ngắn gọn, trung thực và chính xác. Tránh trả lời các câu hỏi không có trong thông tin mà bạn được cung cấp.\n",
    "Nếu câu hỏi không liên quan đến nội dung mà bạn được cung cấp, bạn hãy trả lời rằng bạn không biết câu trả lời. Không được sử dụng những kiến thức\n",
    "mà bạn đã học để tạo ra câu trả lời\\n{context}<|im_end|>\\n<|im_start|>user\\n{question}<|im_end|>\\n<|im_start|>assitant\"\"\",\n",
    "\n",
    "\"\"\"<|im_start|>system\\nBạn là một trợ lý AI hữu ích. Bạn chỉ sử dụng những thông tin mà bạn được cung cấp để trả lời các câu hỏi,\n",
    "hãy trả lời câu hỏi một cách ngắn gọn, trung thực và chính xác. Tránh trả lời các câu hỏi không có trong thông tin mà bạn được cung cấp.\n",
    "Nếu câu hỏi không liên quan đến nội dung mà bạn được cung cấp, bạn hãy trả lời rằng \"Tôi không biết câu trả lời\".Tuyệt đối không được sử dụng những kiến thức\n",
    "mà bạn đã học để tạo ra câu trả lời\\n{context}<|im_end|>\\n<|im_start|>user\\n{question}<|im_end|>\\n<|im_start|>assitant\"\"\",\n",
    "\n",
    "\"\"\"<|im_start|>system\\nBạn là một bậc thầy trong công việc làm trợ lý ảo. Bạn chỉ được sử dụng những thông tin mà bạn được cung cấp để trả lời các câu hỏi,\n",
    "hãy trả lời câu hỏi một cách ngắn gọn, trung thực và chính xác. Tránh trả lời các câu hỏi không có trong thông tin mà bạn được cung cấp.\n",
    "Nếu câu hỏi không liên quan đến nội dung mà bạn được cung cấp, bạn hãy trả lời rằng \"Tôi không biết câu trả lời\". Bỏ qua những tìm kiếm ngoại lệ không liên quan đến câu hỏi.\n",
    "Không được sử dụng những kiến thức mà bạn đã học để bịa ra câu trả lời\\n{context}<|im_end|>\\n<|im_start|>user\\n{question}<|im_end|>\\n<|im_start|>assitant\"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c79646d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
